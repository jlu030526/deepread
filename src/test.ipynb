{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import load_from_pickle, Model\n",
    "import os\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model\n"
     ]
    }
   ],
   "source": [
    "data = load_from_pickle('./data')\n",
    "model_path = './output/lipReaderModel.keras'\n",
    "\n",
    "if os.path.exists(model_path):\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    print(\"Loading saved model\")\n",
    "else:\n",
    "    model = Model(len(data[\"idx2word\"].keys()), hidden_size=32, window_size=20)\n",
    "    print(\"Creating new model\")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.005), \n",
    "    # metrics=[acc_metric],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Valid 14/14]\t loss=35281.285\t acc: 0.034\t perp: inf\n",
      "epoch:0, train_acc:(35281.28515625, 0.03356596827507019, inf)\n",
      "[Valid 14/14]\t loss=41533.199\t acc: 0.040\t perp: inf\n",
      "epoch 0, test acc(41533.19921875, 0.040163375437259674, inf)\n",
      "[Valid 14/14]\t loss=25854.203\t acc: 0.059\t perp: inf\n",
      "epoch:1, train_acc:(25854.203125, 0.05898033827543259, inf)\n",
      "[Valid 14/14]\t loss=24323.879\t acc: 0.225\t perp: inf\n",
      "epoch 1, test acc(24323.87890625, 0.22543352842330933, inf)\n",
      "[Valid 14/14]\t loss=14758.586\t acc: 0.210\t perp: inf\n",
      "epoch:2, train_acc:(14758.5859375, 0.20967741310596466, inf)\n",
      "[Valid 14/14]\t loss=13142.505\t acc: 0.275\t perp: inf\n",
      "epoch 2, test acc(13142.5048828125, 0.27460962533950806, inf)\n"
     ]
    }
   ],
   "source": [
    "epochs = 3\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_acc = model.train(\n",
    "        tf.convert_to_tensor(data[\"train_captions\"]), \n",
    "        tf.convert_to_tensor(data[\"train_videos\"]), \n",
    "        data[\"train_video_mappings\"], data[\"word2idx\"]['<pad>'], batch_size=15)\n",
    "    print(f\"epoch:{e}, train_acc:{train_acc}\")\n",
    "    test_acc = model.test(\n",
    "        tf.convert_to_tensor(data[\"test_captions\"]), \n",
    "        tf.convert_to_tensor(data[\"test_videos\"]), \n",
    "        data[\"test_video_mappings\"], data[\"word2idx\"]['<pad>'], batch_size=15\n",
    "    )\n",
    "    print(f\"epoch {e}, test acc{test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INDEED A INDEED PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE\n",
      "INDEED A INDEED PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE\n",
      "INDEED A INDEED PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE\n",
      "INDEED A INDEED PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE PLACE\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gen_caption_temperature(model, image_embedding, wordToIds, padID, temp, window_length):\n",
    "    \"\"\"\n",
    "    Function used to generate a caption using an ImageCaptionModel given\n",
    "    an image embedding. \n",
    "    \"\"\"\n",
    "    idsToWords = {id: word for word, id in wordToIds.items()}\n",
    "    unk_token = wordToIds['<unk>']\n",
    "    caption_so_far = [wordToIds['<start>']]\n",
    "    while len(caption_so_far) < window_length and caption_so_far[-1] != wordToIds['<end>']:\n",
    "        caption_input = np.array([caption_so_far + ((window_length - len(caption_so_far)) * [padID])])\n",
    "        logits = model(np.expand_dims(image_embedding, 0), caption_input)\n",
    "        logits = logits[0][len(caption_so_far) - 1]\n",
    "        probs = tf.nn.softmax(logits / temp).numpy()\n",
    "        next_token = unk_token\n",
    "        attempts = 0\n",
    "        while next_token == unk_token and attempts < 5:\n",
    "            next_token = np.random.choice(len(probs), p=probs)\n",
    "            attempts += 1\n",
    "        caption_so_far.append(next_token)\n",
    "    return ' '.join([idsToWords[x] for x in caption_so_far][1:-1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csci1470",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
